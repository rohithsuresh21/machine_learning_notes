HYPERPARAMETER TUNING
 
---Scikit-Learn provides a variety of tools to help you tune the hyperparameters of your machine-learning models. A popular method is to use grid search.

Think of it like an "Auto-Tune" for your model. Instead of you manually changing max_depth to 4, then 5, then 6 to see which is better, GridSearchCV will:

Take a list of possible values (e.g., max_depth: [3, 6, 9]).

Try every single combination automatically.

Tell you exactly which combination gives the best score.

---
1. How the "Grid" Works
Imagine a table (a grid). On one side, we list different values for max_depth. On the top, we list different learning_rates. GridSearchCV will train a model for every single intersection on that table to find the winner.

2. The Code for Auto-Tuning
Here is how you implement it. This code will automatically find the best "knobs" for your bank loan model.

code:

from sklearn.model_selection import GridSearchCV

# 1. Define the "Menu" of options (the Parameter Grid)
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.01, 0.05],
    'n_estimators': [50, 100, 200]
}

# 2. Initialize the Grid Search
# cv=5 means "Cross-Validation": it tests each setting 5 times on different data chunks
grid_search = GridSearchCV(
    estimator=XGBClassifier(eval_metric='logloss'),
    param_grid=param_grid,
    cv=5, 
    scoring='recall', # We prioritize RECALL to catch risky borrowers!
    verbose=1
)

# 3. Run the experiment
grid_search.fit(X_train_res, y_train_res)

# 4. See the results
print(f"Best Parameters: {grid_search.best_params_}")
print(f"Best Score: {grid_search.best_score_}")

# 5. Use the best model for final predictions
best_model = grid_search.best_estimator_
final_pred = best_model.predict(X_test)

---------------------------------
full code:

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# ML Essentials
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay
from imblearn.over_sampling import SMOTE
from xgboost import XGBClassifier

# 1. SETUP DATA (Assuming X and y are already defined from Stage 1)
# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 2. STAGE 2: BALANCE DATA (SMOTE)
# We only balance the training set to keep the test set realistic
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# 3. STAGE 4: GRID SEARCH SETUP
# We define the "knobs" we want to test
param_grid = {
    'max_depth': [3, 5, 7],
    'learning_rate': [0.1, 0.01, 0.05],
    'n_estimators': [50, 100, 150],
    'subsample': [0.8, 1.0] # Percentage of data used per tree
}

# Initialize the "King"
xgb = XGBClassifier(eval_metric='logloss', use_label_encoder=False)

# Setup GridSearchCV
# scoring='f1' is great because it balances Precision and Recall
grid_search = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid,
    cv=3,           # 3-fold cross-validation
    scoring='f1',   
    verbose=1,      # Shows progress
    n_jobs=-1       # Uses all your CPU cores for speed
)

# 4. RUN THE OPTIMIZATION
print("Starting Grid Search... this may take a minute.")
grid_search.fit(X_train_res, y_train_res)

# 5. EVALUATE THE BEST MODEL
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)

print(f"\nBest Parameters Found: {grid_search.best_params_}")
print("\n--- Final Classification Report ---")
print(classification_report(y_test, y_pred))

# 6. VISUALIZE THE RESULTS
fig, ax = plt.subplots(1, 2, figsize=(15, 5))

# Plot Confusion Matrix
ConfusionMatrixDisplay.from_estimator(best_model, X_test, y_test, ax=ax[0], cmap='Blues')
ax[0].set_title("Confusion Matrix (Final Exam)")

# Plot Feature Importance
feat_importances = pd.Series(best_model.feature_importances_, index=X.columns)
feat_importances.nlargest(10).plot(kind='barh', ax=ax[1])
ax[1].set_title("Top 10 Features (Why the model decided)")

plt.tight_layout()
plt.show()


QUESTION AND ANSWERS:

1. so if there is 1000 accecpted loan and only 100 rejected load the smote creates so synthetic classes for rejected class to balance the accepted loan?
ans: 
	Yes, exactly. If you have 1000 Approved and 100 Rejected, SMOTE will generate 900 new synthetic "Rejected" data points.

It doesn't just copy the old ones. It looks at the 100 real rejected loans and creates "mathematical variations" of them.

After SMOTE, your training set will have exactly 1000 Approved and 1000 Rejected loans. It creates a "fair" environment for the model to learn.

2.in the optimization part grid_search_fit(X_train_res,y_train_res) is making a a grid which involves smote of x and y train which basically balanced training data?
ans:
	Yes. When you run grid_search.fit(X_train_res, y_train_res), the grid is exploring different XGBoost settings using the balanced data.

By using the resampled data (SMOTE), the Grid Search is finding the best "knobs" (like max_depth) for a model that isn't biased.

It ensures that the "Best Parameters" found are the ones that are best at catching both Approved and Rejected loans equally.

3.In evaluation part we just mentioned best estimator what about learning rate and others? 
ans:
	When you use best_model = grid_search.best_estimator_, it actually includes all those settings.

The best_estimator_ is a "packaged" model that already has the winning learning_rate, max_depth, and n_estimators saved inside it.

You don't have to type them again. If the Grid Search found that learning_rate=0.01 was the winner, that 0.01 is already "baked into" the best_model object.

4. "y_pred = best_model.predict(X_test) " this line basically predicts from the question paper (X_test) ?
ans:
	Spot on. * X_test is the question paper (the features).

best_model.predict(X_test) is the model writing its answers (guesses) on a new sheet of paper.

That sheet of paper with the guesses is called y_pred

5. What is "Scoring" in para_grid?
ans:
	"Scoring" tells the Grid Search how to pick the winner.

If you set scoring='accuracy', the grid will pick the model with the most correct guesses.

If you set scoring='f1' (which I recommend for banks), it tells the grid: "Don't just look for high accuracy; find the model that balances catching bad loans (Recall) with not annoying good customers (Precision)."

Itâ€™s basically the "Judge" that decides which parameter combination gets the gold medal.

