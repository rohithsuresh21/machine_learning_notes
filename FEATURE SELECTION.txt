FEATURE SELECTION

1. Filter Methods:
evaluate each feature independently with target variable. Feature with high correlation with target variable are selected as it means this feature has some relation and can help us in making predictions.

Common Filter Techniques

Information Gain: Measures reduction in entropy when a feature is used.
Chi-square test: Checks the relationship between categorical features.
Fisher’s Score: Ranks features based on class separability.
Pearson’s Correlation Coefficient: Measures linear relationship between two continuous variables.
Variance Threshold: Removes features with very low variance.
Mean Absolute Difference: Similar to variance threshold but uses absolute differences.
Dispersion ratio: Ratio of arithmetic mean to geometric mean; higher values indicate useful features.

Advantages

Fast and efficient: Filter methods are computationally inexpensive, making them ideal for large datasets.
Easy to implement: These methods are often built-in to popular machine learning libraries, requiring minimal coding effort.
Model Independence: Filter methods can be used with any type of machine learning model, making them versatile tools.

Limitations

Limited interaction with the model: Since they operate independently, filter methods might miss data interactions that could be important for prediction.
Choosing the right metric: Selecting the appropriate metric for our data and task is important for optimal performance.

2. Wrapper methods
Wrapper methods are also referred as greedy algorithms that train algorithm. They use different combination of features and compute relation between these subset features and target variable and based on conclusion addition and removal of features are done. Stopping criteria for selecting the best subset are usually pre-defined by the person training the model such as when the performance of the model decreases or a specific number of features are achieved.


Common Wrapper Techniques

Forward Selection: Start with no features and add one at a time based on improvement.
Backward Elimination: Start with all features and remove the least useful ones.
Recursive Feature Elimination (RFE): Removes the least important features step by step.

Advantages
Model-specific optimization: Wrapper methods directly consider how features influence the model, potentially leading to better performance compared to filter methods.
Flexible: These methods can be adapted to various model types and evaluation metrics.

Limitations
Computationally expensive: Evaluating different feature combinations can be time-consuming, especially for large datasets.
Risk of overfitting: Fine-tuning features to a specific model can lead to an overfitted model that performs poorly on unseen data.


3. Embedded methods
Embedded methods perform feature selection during the model training process. They combine the benefits of both filter and wrapper methods. Feature selection is integrated into the model training allowing the model to select the most relevant features based on the training process dynamically.


Common Embedded Techniques

L1 Regularization (Lasso): Keeps only features with non-zero coefficients.
Decision Trees and Random Forests: Select features based on impurity reduction.
Gradient Boosting: Pick features that reduce prediction error the most


Advantages

Efficient and effective: Embedded methods can achieve good results without the computational burden of some wrapper methods.
Model-specific learning: Similar to wrapper methods these techniques uses the learning process to identify relevant features.

Limitations

Limited interpretability: Embedded methods can be more challenging to interpret compared to filter methods making it harder to understand why specific features were chosen.
Not universally applicable: Not all machine learning algorithms support embedded feature selection techniques.
Choosing the Right Feature Selection Method

Choice of feature selection method depends on several factors:
Dataset size: Filter methods are generally faster for large datasets while wrapper methods might be suitable for smaller datasets.
Model type: Some models like tree-based models, have built-in feature selection capabilities.
Interpretability: If understanding the rationale behind feature selection is crucial, filter methods might be a better choice.
Computational resources: Wrapper methods can be time-consuming, so consider our available computing power.



further learning: https://www.geeksforgeeks.org/machine-learning/feature-selection-techniques-in-machine-learning/